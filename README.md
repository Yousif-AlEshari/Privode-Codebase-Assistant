# Private Codebase Assistant (RAG System)

A multi-project **Retrieval-Augmented Generation (RAG)** system that lets teams **query their private repositories** through natural-language questions.
Built with **FastAPI**, **Streamlit**, **ChromaDB**, and **Groq LLM (LLaMA 3)** for blazing-fast and secure in-house code understanding.

---

## Features

 **Automatic Repository Ingestion**

- Clone or upload repositories.
- Parse source files into semantic chunks with metadata (path, file type, etc.).
- Store embeddings persistently in ChromaDB.

 **Multi-Project Management**

- Create, list, and delete projects.
- Re-embed or update code incrementally.

 **Intelligent Question Answering**

- Ask natural-language questions about your code.
- Retrieves top-K most relevant chunks using vector similarity.
- LLaMA 3 (Groq API) generates concise, code-aware answers with citations.

 **FastAPI + Streamlit Interface**

- **Backend**: `/projects`, `/ask`, `/search`, `/ingest`, `/browse` endpoints.
- **Frontend**: A clean Streamlit dashboard to:
  - Manage projects.
  - Ingest or upload code.
  - Ask questions with or without LLM.
  - Browse files, documents, and chunks interactively.

 **Persistent Local Vector Store**

- All embeddings are stored in `data/chroma_store/` using `chromadb.PersistentClient`.

---

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ingest_repo.pyâ”‚  â† Clone or read files
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ parse_chunk.pyâ”‚  â† Split files â†’ text chunks + metadata
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ embed_store.pyâ”‚  â† Store / query embeddings in ChromaDB
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ retrieval.py  â”‚  â† Query top-K relevant chunks
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ answer_generation.py â”‚  â† LLM (Groq) answers using context
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Frontend â†’ **Streamlit app.py**  
Backend â†’ **FastAPI api_server.py**
```

---

## Repository Structure

```
src/
 â”œâ”€â”€ pipeline/
 â”‚   â”œâ”€â”€ ingest_repo.py       # Clone or walk folders, parse + upsert chunks
 â”‚   â”œâ”€â”€ parse_chunk.py       # Chunk text & attach metadata
 â”‚   â”œâ”€â”€ embed_store.py       # Persistent ChromaDB store & retrieval utils
 â”‚   â”œâ”€â”€ retrieval.py         # Query + generate answers
 â”‚   â”œâ”€â”€ answer_generation.py # Embeddings + LLM (Groq API)
 â”‚
 â”œâ”€â”€ services/
 â”‚   â”œâ”€â”€ api_server.py        # FastAPI backend (project CRUD + /ask)
 â”‚   â”œâ”€â”€ app.py               # Streamlit frontend dashboard
 â”‚
data/
 â”œâ”€â”€ chroma_store/            # Persistent vector database
 â”œâ”€â”€ projects.json            # Registered project metadata
 â”œâ”€â”€ uploads/                 # Uploaded files storage
 â””â”€â”€ parsed_chunks.json       # Example output
```

---

## Installation

### 1. Clone the repo

```bash
git clone https://github.com/<your-username>/<repo-name>.git
cd <repo-name>
```

### 2. Create a virtual environment

```bash
python -m venv venv
source venv/bin/activate    # or venv\Scripts\activate on Windows
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Environment variables

Create a `.env` file in the root:

```
LLM_key=your_groq_api_key
API_BASE=http://localhost:8000
```

---

## Running the System

### Start the **FastAPI backend**

```bash
uvicorn src.services.api_server:app --reload --port 8000
```

### Start the **Streamlit frontend**

```bash
streamlit run src.services.app.py
```

Access it at  **http://localhost:8501**

---

## Example Usage

1. **Create a new project**
   - From the *Projects* tab â†’ â€œCreate New Projectâ€ â†’ give it a name.
2. **Ingest your repo**
   - Either clone from GitHub or specify a local folder.
3. **Ask a question**
   - Go to *Ask* tab â†’ select your project â†’ ask:
     _â€œHow are sentences split in our code?â€_
4. **View matches & answer**
   - See retrieved code chunks and the AIâ€™s summarized explanation.

---

## Tech Stack

| Layer       | Technology                                                   |
| ----------- | ------------------------------------------------------------ |
| Backend API | **FastAPI**                                            |
| Frontend UI | **Streamlit**                                          |
| Vector DB   | **ChromaDB (PersistentClient)**                        |
| Embeddings  | **SentenceTransformer â€“ all-MiniLM-L6-v2**            |
| LLM Engine  | **Groq API (LLaMA 3 / Mixtral)**                       |
| Environment | **Python 3.10+**, `.env`, `requests`, `pydantic` |

---

## Example Data Paths

- `data/projects.json` â€” metadata for registered projects
- `data/chroma_store/` â€” vector database
- `data/parsed_chunks.json` â€” sample parsed output
- `data/uploads/` â€” temporary uploaded files

---

## Future Enhancements

- Fine-tuned code understanding for different languages.
- User authentication for team collaboration.
- Streamed LLM responses and syntax-highlighted answers.
- Hybrid retrieval (text + AST embeddings).
- Integration with enterprise Git services (GitLab, Bitbucket, etc.).

---

## License

This project is released under the **MIT License** â€” feel free to use, modify, and adapt it for your internal systems.

---

### ğŸ‘¤ Author

**Yousif Al Eshari** â€” Al-Hussein Technical University
 Developer & Researcher |  Building secure private RAG assistants
